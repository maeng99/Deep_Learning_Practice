{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"bJm88P18Mx4b"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('drive/MyDrive/DL2024_1/Attention')\n","\n","%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","source":["import os\n","import shutil\n","import zipfile\n","\n","import pandas as pd\n","import tensorflow as tf\n","import urllib3\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"53s2Qur-M06T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lines = pd.read_csv('dataset/fra.txt', names=['src', 'tar', 'lic'], sep='\\t')\n","del lines['lic']\n","print('전체 샘플의 개수 :',len(lines))"],"metadata":{"id":"03HWuIAgM6Ob"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lines = lines.loc[:, 'src':'tar']\n","lines = lines[0:30000] # 3만개만 사용\n","lines.sample(10)"],"metadata":{"id":"BpsaTkOVM8JJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lines.tar = lines.tar.apply(lambda x : '\\t '+ x + ' \\n')\n","lines.sample(10)"],"metadata":{"id":"Buq3e7a7M9Oe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 문자 집합 구축\n","src_vocab = set()\n","for line in lines.src: # 1줄씩 읽음\n","    for char in line: # 1개의 문자씩 읽음\n","        src_vocab.add(char)\n","\n","tar_vocab = set()\n","for line in lines.tar:\n","    for char in line:\n","        tar_vocab.add(char)\n","src_vocab_size = len(src_vocab)+1\n","tar_vocab_size = len(tar_vocab)+1\n","print('source 문장의 char 집합 :',src_vocab_size)\n","print('target 문장의 char 집합 :',tar_vocab_size)\n"],"metadata":{"id":"2rZibdEGM-9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["src_vocab = sorted(list(src_vocab))\n","tar_vocab = sorted(list(tar_vocab))\n","print(src_vocab[45:75])\n","print(tar_vocab[45:75])\n"],"metadata":{"id":"y-qwdJ-nNAIO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["src_to_index = dict([(word, i+1) for i, word in enumerate(src_vocab)])\n","tar_to_index = dict([(word, i+1) for i, word in enumerate(tar_vocab)])\n","print(src_to_index)\n","print(tar_to_index)"],"metadata":{"id":"B1UgHvDvNBQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_input = []\n","\n","# 1개의 문장\n","for line in lines.src:\n","  encoded_line = []\n","  # 각 줄에서 1개의 char\n","  for char in line:\n","    # 각 char을 정수로 변환\n","    encoded_line.append(src_to_index[char])\n","  encoder_input.append(encoded_line)\n","print('source 문장의 정수 인코딩 :',encoder_input[:5])\n"],"metadata":{"id":"5WW764AINCU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoder_input = []\n","for line in lines.tar:\n","  encoded_line = []\n","  for char in line:\n","    encoded_line.append(tar_to_index[char])\n","  decoder_input.append(encoded_line)\n","print('target 문장의 정수 인코딩 :',decoder_input[:5])\n"],"metadata":{"id":"hOOmmNN9NDlb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["decoder_target = []\n","for line in lines.tar:\n","  timestep = 0\n","  encoded_line = []\n","  for char in line:\n","    if timestep > 0:\n","      encoded_line.append(tar_to_index[char])\n","    timestep = timestep + 1\n","  decoder_target.append(encoded_line)\n","print('target 문장 레이블의 정수 인코딩 :',decoder_target[:5])\n"],"metadata":{"id":"CvvpMMeLNE9r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_src_len = max([len(line) for line in lines.src])\n","max_tar_len = max([len(line) for line in lines.tar])\n","print('source 문장의 최대 길이 :',max_src_len)\n","print('target 문장의 최대 길이 :',max_tar_len)\n"],"metadata":{"id":"AONv69nINHMx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_input = pad_sequences(encoder_input, maxlen=max_src_len, padding='post')\n","decoder_input = pad_sequences(decoder_input, maxlen=max_tar_len, padding='post')\n","decoder_target = pad_sequences(decoder_target, maxlen=max_tar_len, padding='post')\n","encoder_input = to_categorical(encoder_input)\n","decoder_input = to_categorical(decoder_input)\n","decoder_target = to_categorical(decoder_target)\n"],"metadata":{"id":"zOh6zEstNIcN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Attention layer를 추가한 Seq2Seq 모델 학습해보기"],"metadata":{"id":"ybIq2SyJUk-0"}},{"cell_type":"code","source":["from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Dot, Activation, Lambda, Softmax\n","from tensorflow.keras.optimizers import RMSprop\n","import numpy as np\n","import tensorflow as tf"],"metadata":{"id":"NLLUpNPeNLlZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 어텐션 레이어\n","class AttentionLayer(tf.keras.layers.Layer):\n","    def __init__(self):\n","        super(AttentionLayer, self).__init__()\n","\n","    def call(self, query, key, value):\n","        scores = tf.matmul(query, key, transpose_b=True)\n","        attention_weights = Softmax(axis=-1)(scores)\n","        context_vector = tf.matmul(attention_weights, value)\n","        return context_vector, attention_weights"],"metadata":{"id":"SliYNzs2NOBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 인코더 정의\n","encoder_inputs = Input(shape=(None, src_vocab_size))\n","encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n","encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n","\n","# 디코더 정의\n","decoder_inputs = Input(shape=(None, tar_vocab_size))\n","decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n"],"metadata":{"id":"IWbDAxMJNupR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 어텐션 레이어 추가\n","# 1) AttentioLayer  선언\n","# 2) context_vector, attention_weights 에 출력 담기\n","'''\n","\n","\n","'''\n","\n","# 컨텍스트 벡터와 디코더 출력을 연결\n","decoder_concat_input = Concatenate(axis=-1)([context_vector, decoder_outputs])\n","\n","# 출력 레이어\n","decoder_dense = Dense(tar_vocab_size, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_concat_input)\n","\n","# 전체 모델 정의\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","\n","# 모델 컴파일\n","model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 요약\n","model.summary()"],"metadata":{"id":"zkdhQz6uNwi8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 모델 학습\n","model.fit(\n","    [encoder_input, decoder_input],\n","    decoder_target,\n","    batch_size=64,\n","    epochs=40,\n","    validation_split=0.2\n",")\n","\n"],"metadata":{"id":"Bu1uJuRXNz8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index_to_src = dict((i, char) for char, i in src_to_index.items())\n","index_to_tar = dict((i, char) for char, i in tar_to_index.items())\n"],"metadata":{"id":"kPIrjojKUaQO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 인코더 모델\n","encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n","\n","# 디코더\n","# 입력 정의\n","decoder_state_input_h = Input(shape=(256,), name=\"decoder_state_input_h\")\n","decoder_state_input_c = Input(shape=(256,), name=\"decoder_state_input_c\")\n","\n","# 디코더 LSTM\n","decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=[decoder_state_input_h, decoder_state_input_c])\n","\n","# 어텐션 레이어 추가\n","context_vector, attention_weights = attention_layer(decoder_outputs, encoder_outputs, encoder_outputs)\n","\n","# 컨텍스트 벡터와 디코더 출력을 결합\n","decoder_concat_input = Concatenate(name=\"concatenate_layer\")([context_vector, decoder_outputs])\n","\n","# 최종 출력 레이어\n","decoder_final_output = decoder_dense(decoder_concat_input)\n","\n","# 디코더 모델 생성\n","decoder_model = Model(\n","    inputs=[decoder_inputs, encoder_outputs, decoder_state_input_h, decoder_state_input_c],\n","    outputs=[decoder_final_output, state_h, state_c, attention_weights]\n",")\n","\n","\n","# 오류 메시지에 나타난 문제를 디버깅하기 위해 모델의 개요를 출력\n","encoder_model.summary()\n","decoder_model.summary()\n","\n"],"metadata":{"id":"2HOlvfvMU_GL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 번역 결과를 디코딩하는 함수\n","def decode_sequence(input_seq):\n","    # 인코더의 상태를 얻음\n","    encoder_output, state_h, state_c = encoder_model.predict(input_seq)\n","\n","    # 디코더의 초기 입력 (시작 심볼)\n","    target_seq = np.zeros((1, 1, tar_vocab_size))\n","    target_seq[0, 0, tar_to_index['\\t']] = 1.\n","\n","    # 디코딩 루프\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c, a = decoder_model.predict([target_seq, encoder_output, state_h, state_c])\n","\n","        # 샘플링\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = index_to_tar[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # 종료 조건: 최대 길이 초과 또는 종료 심볼\n","        if (sampled_char == '\\n' or len(decoded_sentence) > max_tar_len):\n","            stop_condition = True\n","\n","        # 다음 디코더 입력 업데이트\n","        target_seq = np.zeros((1, 1, tar_vocab_size))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # 상태 업데이트\n","        state_h, state_c = h, c\n","\n","    return decoded_sentence"],"metadata":{"id":"wC1yCZVkVVO2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 테스트 데이터 사용 예시\n","for seq_index in [3,50,100,300,1001]: # 입력 문장의 인덱스\n","  input_seq = encoder_input[seq_index:seq_index+1]\n","  decoded_sentence = decode_sequence(input_seq)\n","  print(35 * \"-\")\n","  print('입력 문장:', lines.src[seq_index])\n","  print('정답 문장:', lines.tar[seq_index][2:len(lines.tar[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n","  print('번역 문장:', decoded_sentence[1:len(decoded_sentence)-1]) # '\\n'을 빼고 출력\n"],"metadata":{"id":"y5VvDPvnOwSk"},"execution_count":null,"outputs":[]}]}